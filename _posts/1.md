#机器学习面试题总结（1）
###导语：
> 为了准备暑期实习，从各种面经中总结了一些机器学习的面试题。答案一部分是网上的答案，一部分是自己的心得。这些东西类似“真题”，一来检测自己的知识掌握情况，二来作为之后的学习做一个导向。

***
####***Q1：如何处理缺失值，以及各种方法的利弊？***
1.用平均值、中值、众数、随机数等替代。优点是简便。缺点是相当于人为增加了噪声

2.用其他变量做一个预测模型来算出缺失值。但是如果其他变量与缺失变量若无关，则计算无意义；若强相关，则不必计算

3.把缺失值当做一个独立的类型，然后把该变量映射到一个高维的空间（类似[one-hot](https://en.wikipedia.org/wiki/One-hot)）。优点是可以保持样本的所有信息。缺点是计算量巨大，且如果样本数目较少的话，这样映射会使得数据变得非常稀疏。
####***Q2：如何将描述变量转为连续变量？***

描述变量也叫分类变量，一般是离散的。可以用其拟合一个回归方程，然后可当做连续变量处理。
####***Q3:如何处理有序变量？***
有序变量最好做一下one-hot encoding（独热编码），也可以将有序变量直接当做连续变量。

####***Q4：如何进行特征选择？***
从给定的特征集合中选择出相关特征子集的过程，称为[特征选择](https://baike.baidu.com/item/特征选择/4950639?fr=aladdin)。特征选择和降维是处理高维数据的两大主流技术。其主要涉及两个关键环节：`子集搜索`和`子集评价`。常见的方法有三大类：

1.**过滤式**

 过滤方式先对数据集进行特征选择，然后再训练学习器。**过滤式不考虑后续学习器的最终性能**。Relief是一种著名的过滤式特征选择方法，设计了一个“相关统计量”来度量特征的重要性。相关统计量对应于属性 $j$ 的分量为：

$$\sigma^j = \sum_i-diff(x_i^j,x_{i,nh}^j)^2 + diff(x_i^j,x_{i,nm}^j)^2$$

其中$x_a^j$表示样本***$x_a$***在属性j上的取值，$diff(x_a^j,x_b^j)$取决于属性j的类型：若是离散型，则$x_a^j=x_b^j$时取0，否则取1；若是连续型，则其值等于$\left| x_a^j-x_b^j \right|$，注意$x_a^j,x_b^j$已规范化到[0,1]区间内。

Relief算法是针对二分类问题的，而多分类问题要用到Relief-F算法。该算法中，相关变量对于属性 $j$ 的分量为：

$$\sigma^j = \sum_i - diff(x_i^j, x_{i,nh}^j)^2 + \sum_{l\neq k}(p_l \times diff(x_i^j, x_{i,nm}^j)^2)​$$

其中$p_l$为第 $l$ 类样本在数据集 $D$ 中所占的比例。

2.**包裹式**

包裹式考虑了特征子集选择对学习器效果的影响，选择出一个使学习器效果最佳的特征子集。因为在特征选择中需要多次训练学习器，所以计算开销较大。LVW是一个典型的包裹式特征选择方法。算法描述如下：















